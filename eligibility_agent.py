# eligibility_agent.py
import os
import sys
import json
import asyncio
from pathlib import Path

from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from openai import OpenAI
import logging   

# ===== é—œé–‰ç¬¬ä¸‰æ–¹æ¨¡å‹çš„ INFO log=====
logging.getLogger("sentence_transformers").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("torch").setLevel(logging.ERROR)
# ==========================================================

# ==========================================
# 1. åˆå§‹åŒ–ç’°å¢ƒèˆ‡è¨­å®š
# ==========================================
DATA_PATH = Path(__file__).parent / "cards_rag_embedded.jsonl"
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = os.getenv(
    "GEMINI_BASE_URL",
    "https://generativelanguage.googleapis.com/v1beta/openai/"
)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")

try:
    llm_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url=GEMINI_BASE_URL,
    )
except Exception as e:
    print(f"âŒ Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
    llm_client = None

mcp = FastMCP("eligibility-agent")

# ==========================================
# 2. è¼•é‡ç‰ˆï¼šäº”å¼µå¡çš„ç”³è¾¦è¦å‰‡è¡¨
#   
# ==========================================

try:
    from rag_search import search_chunks, load_index
    load_index()  # ï¼Œå…ˆè¼‰å…¥å‘é‡åº«

    
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° rag_search.pyï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚", file=sys.stderr)
    sys.exit(1)
def list_cards_from_rag() -> list[str]:
    """
    æƒæ cards_rag_embedded.jsonlï¼Œæ‰¾å‡ºæ‰€æœ‰å‡ºç¾éçš„å¡ç‰‡åç¨±ï¼ˆcard_nameï¼‰ï¼Œå»é‡å¾Œå›å‚³åˆ—è¡¨ã€‚
    è‹¥æª”æ¡ˆä¸å­˜åœ¨ï¼Œå›å‚³ç©ºåˆ—è¡¨ã€‚
    """
    card_names: set[str] = set()

    if not DATA_PATH.exists():
        print(f"âš ï¸ æ‰¾ä¸åˆ° {DATA_PATH}ï¼Œç„¡æ³•å¾ RAG æƒæå¡ç‰‡åç¨±ã€‚", file=sys.stderr)
        return []

    with DATA_PATH.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                continue

            name = obj.get("card_name")
            if isinstance(name, str) and name.strip():
                card_names.add(name.strip())

    return sorted(card_names)
    
DEFAULT_CARD_NAMES = list_cards_from_rag()



async def _check_single_card_llm(user: dict, card_name: str) -> dict:
    """
    ä½¿ç”¨ LLM + RAG å…§å®¹åˆ¤æ–· eligibilityã€‚
    âœ… ä¸å†å¼·åˆ¶ LLM è¼¸å‡º JSONï¼Œå› æ­¤ä¸æœƒå†å‡ºç¾ã€Œç„¡æ³•è§£æã€ã€‚
    """

    # 1) å¾ RAG æœå°‹è©²å¡ç‰‡ç›¸é—œå…§å®¹
    try:
        rag_results = await asyncio.to_thread(
            search_chunks,
            query=card_name,
            card_filter=card_name,
            top_k=5,
        )
    except Exception as e:
        rag_results = []
        print(f"âš ï¸ RAG æœå°‹å¤±æ•—ï¼ˆ{card_name}ï¼‰ï¼š{e}", file=sys.stderr)

    # 2) è®“ LLM æ ¹æ“š RAG å…§å®¹åš eligibility æ¨è«–ï¼ˆç”¨è‡ªç„¶èªè¨€å³å¯ï¼‰
    prompt = f"""
ä½ æ˜¯ä¸€ä½ä¿¡ç”¨å¡ç”³è¾¦è³‡æ ¼åˆ†æå°ˆå®¶ã€‚

ä½¿ç”¨è€…è³‡æ–™ï¼ˆJSONï¼‰ï¼š
{json.dumps(user, ensure_ascii=False)}

å¡ç‰‡åç¨±ï¼š{card_name}

ä»¥ä¸‹æ˜¯å¾ RAG æœå°‹åˆ°çš„å¡ç‰‡å…§å®¹ï¼ˆå¯èƒ½åŒ…å«å›é¥‹ã€å„ªæƒ ã€æ¢æ¬¾ã€è³‡æ ¼ç­‰ï¼‰ï¼š
{json.dumps(rag_results, ensure_ascii=False)}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
1) é€™å¼µå¡å°ç”³è«‹äººçš„å¹´é½¡/æ”¶å…¥/å­¸ç”Ÿèº«åˆ†æ˜¯å¦æœ‰æ˜ç¢ºé–€æª»ï¼Ÿï¼ˆè‹¥æ²’æœ‰å¯«å°±èªªã€Œè³‡æ–™ä¸è¶³ã€ï¼‰
2) ä»¥æ­¤ä½¿ç”¨è€…æ¢ä»¶ï¼Œçµ¦ã€Œå»ºè­°ç”³è¾¦ / ä¸å»ºè­° / è³‡è¨Šä¸è¶³ã€å…¶ä¸€
3) 2ï½4 é»æ¢åˆ—ç†ç”±

ç›´æ¥è¼¸å‡ºæ–‡å­—ï¼Œä¸è¦ JSONã€‚
"""

    resp = llm_client.chat.completions.create(
        model=GEMINI_MODEL,
        messages=[{"role": "user", "content": prompt}],
    )

    explanation = (resp.choices[0].message.content or "").strip()

    # 3) æˆ‘å€‘è‡ªå·±åŒ…æˆ dict å›å»ï¼ˆé¿å…è§£æï¼‰
    return {
        "card_name": card_name,
        "status": "LLM_æ¨è«–",
        "reasons": [explanation] if explanation else ["è³‡æ–™ä¸è¶³ï¼Œç„¡æ³•åˆ¤æ–·ã€‚"],
        "rule_notes": "ç”± LLM æ ¹æ“š RAG å…§å®¹æ¨è«–ï¼ˆæœªç¡¬ç·¨ç¢¼è¦å‰‡ï¼‰ã€‚",
    }

async def tool_check_eligibility(user_profile_json: str, card_names: list[str] | None = None) -> str:
    """
    å·¥å…·ï¼šæ ¹æ“š user_profile + å¡ç‰‡æ¸…å–®ï¼Œå›å‚³æ¯å¼µå¡çš„ç”³è¾¦è³‡æ ¼åˆ¤æ–·çµæœã€‚
    """
    try:
        user = json.loads(user_profile_json)
    except Exception as e:
        return json.dumps({"error": f"user_profile_json è§£æå¤±æ•—: {e}"}, ensure_ascii=False)

    if not isinstance(user, dict):
        return json.dumps({"error": "user_profile_json å¿…é ˆæ˜¯ä¸€å€‹ JSON ç‰©ä»¶"}, ensure_ascii=False)

    names = card_names or DEFAULT_CARD_NAMES
    results = []
    for name in names:
        results.append(await _check_single_card_llm(user, name))

    return json.dumps(results, ensure_ascii=False)
# ==========================================
# 3. å·¥å…· Schema èˆ‡ System Prompt
# ==========================================

INTERNAL_TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "tool_check_eligibility",
            "description": "æ ¹æ“šä½¿ç”¨è€…æ¢ä»¶ï¼Œæª¢æŸ¥æŒ‡å®šä¿¡ç”¨å¡æ˜¯å¦ç¬¦åˆç”³è¾¦é–€æª»ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "user_profile_json": {
                        "type": "string",
                        "description": "ä½¿ç”¨è€…è³‡æ–™çš„ JSON å­—ä¸²ï¼Œä¾‹å¦‚ {\"age\":23,\"annual_income\":450000,\"is_student\":false}"
                    },
                    "card_names": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "è¦æª¢æŸ¥çš„å¡ç‰‡åç¨±åˆ—è¡¨ï¼Œè‹¥çœç•¥å‰‡é è¨­ç‚ºç³»çµ±å…§å»ºçš„äº”å¼µå¡ã€‚"
                    }
                },
                "required": ["user_profile_json"]
            }
        }
    }
]

ELIGIBILITY_SYSTEM_PROMPT = """
ä½ æ˜¯ã€Œä¿¡ç”¨å¡ç”³è¾¦è³‡æ ¼å°ˆå®¶ã€ï¼Œè² è²¬åˆ¤æ–·ä½¿ç”¨è€…æ˜¯å¦é©åˆç”³è¾¦ç‰¹å®šä¿¡ç”¨å¡ã€‚

### å·¥å…·ä½¿ç”¨è¦å‰‡
- ä»»ä½•éœ€è¦åˆ¤æ–·ã€Œå¯ä¸å¯ä»¥è¾¦ã€ã€ã€Œéä»¶æ©Ÿç‡é«˜ä¸é«˜ã€ã€ã€Œå“ªå¼µæ¯”è¼ƒå®¹æ˜“ç”³è¾¦ã€çš„å•é¡Œï¼Œ
  éƒ½å¿…é ˆå‘¼å« `tool_check_eligibility`ï¼Œå–å¾—æ¯å¼µå¡çš„æ©Ÿæ¢°å¼åˆ¤æ–·çµæœèˆ‡åŸå› ã€‚

### å›ç­”åŸå‰‡
1. **å…ˆçœ‹çµæ§‹åŒ–çµæœï¼Œå†è£œå……èªªæ˜**ï¼š
   - å…ˆä¾ç…§å·¥å…·å›å‚³çš„ statusï¼ˆâœ…/âŒï¼‰åšæ•´ç†ã€‚
   - å†ç”¨æ¢åˆ—å¼èªªæ˜ç†ç”±ï¼Œä¾‹å¦‚å¹´é½¡ã€å¹´æ”¶ã€å­¸ç”Ÿèº«åˆ†ç­‰ã€‚
2. **ä¸è¦äº‚çŒœéŠ€è¡Œå…§è¦**ï¼š
   - å·¥å…·æ²’æœ‰æä¾›çš„è³‡æ–™ï¼Œå°±èªªã€Œæ­¤éƒ¨åˆ†ä»ä»¥éŠ€è¡Œå¯¦éš›å¯©æ ¸ç‚ºæº–ã€ã€‚
3. **è¼¸å‡ºæ ¼å¼å»ºè­°**ï¼š
   - å…ˆçµ¦ä¸€å€‹ç¸½çµï¼šæ¯”å¦‚ã€Œæ•´é«”ä¾†èªªï¼Œä½ æœ€é©åˆ Aã€B å¡ã€ã€‚
   - å†åˆ—å‡ºæ¯å¼µå¡ï¼šå¡å / å»ºè­° / åŸå› ï¼ˆæ¢åˆ—ï¼‰ã€‚
4. **user_profile ä¾†æº**ï¼š
   - ä½ æœƒæ”¶åˆ°ä¸€å€‹ `user_profile` å­—ä¸²åƒæ•¸ï¼Œå¯ç›´æ¥ç•¶ä½œ JSONï¼Œ
     ä¹Ÿå¯ä»¥ä¾ä½¿ç”¨è€…åœ¨å°è©±ä¸­è£œå……çš„è³‡è¨Šåšå£é ­è§£é‡‹ã€‚

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
"""

# ==========================================
# 4. REACT LOOP
# ==========================================

async def _generate_response(user_query: str, user_profile: str = "") -> str:
    if not llm_client:
        return "âŒ ç³»çµ±éŒ¯èª¤ï¼šLLM client æœªåˆå§‹åŒ–"

    messages = [
        {"role": "system", "content": ELIGIBILITY_SYSTEM_PROMPT},
        {
            "role": "user",
            "content": (
                f"ä½¿ç”¨è€…è³‡æ–™(user_profile JSON)ï¼š{user_profile}\n"
                f"ä½¿ç”¨è€…å•é¡Œï¼š{user_query}"
            ) if user_profile else user_query
        },
    ]

    MAX_TURNS = 5
    turn = 0

    try:
        while turn < MAX_TURNS:
            turn += 1

            resp = llm_client.chat.completions.create(
                model=GEMINI_MODEL,
                messages=messages,
                tools=INTERNAL_TOOLS_SCHEMA,
                tool_choice="auto",
            )

            msg = resp.choices[0].message
            messages.append(msg)

            # æ²’æœ‰å†å‘¼å«å·¥å…· â†’ ç›´æ¥å›è¦†
            if not msg.tool_calls:
                return msg.content

            # åŸ·è¡Œå·¥å…·
            for tool_call in msg.tool_calls:
                fname = tool_call.function.name
                args = json.loads(tool_call.function.arguments)

                if fname == "tool_check_eligibility":
                    tool_result = await tool_check_eligibility(**args)
                else:
                    tool_result = json.dumps(
                        {"error": f"Unknown tool: {fname}"},
                        ensure_ascii=False
                    )

                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": fname,
                    "content": tool_result,
                })

        return "âš ï¸ è¶…éæ€è€ƒæ¬¡æ•¸ä¸Šé™ï¼Œç„¡æ³•å–å¾—å®Œæ•´è³‡è¨Šã€‚"

    except Exception as e:
        return f"âŒ Agent åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}"

# ==========================================
# 5. MCP Tool Entry
# ==========================================

@mcp.tool()
async def eligibility_agent(user_query: str, user_profile: str = "") -> str:
    """
    ä¸»è¦é€²å…¥é»ï¼šæª¢æŸ¥æŒ‡å®šå¡ç‰‡çš„ç”³è¾¦è³‡æ ¼ã€‚
    - user_query: ä½¿ç”¨è€…è‡ªç„¶èªè¨€å•é¡Œ
    - user_profile: å»ºè­°å‚³ JSON å­—ä¸²ï¼Œä¾‹å¦‚ {"age":23,"annual_income":450000,"is_student":false}
    """
    print(f"ğŸªª [Eligibility Agent] æ”¶åˆ°è«‹æ±‚ | Query={user_query}", file=sys.stderr)
    return await _generate_response(user_query, user_profile)


# ==========================================
# 6. Local æ¸¬è©¦æ¨¡å¼
# ==========================================

async def local_chat_loop():
    print("\nğŸªª --- Eligibility Agent Local Mode ---")
    print("è¼¸å…¥ 'q' é›¢é–‹")

    profile = input("è«‹è¼¸å…¥ user_profile JSON (å¯ç•™ç©º): ").strip()
    if not profile:
        # çµ¦ä¸€å€‹ç¤ºç¯„ç”¨ profile
        profile = json.dumps(
            {"age": 23, "annual_income": 450000, "is_student": False},
            ensure_ascii=False,
        )
        print(f"ğŸ‘‰ ä½¿ç”¨é è¨­ profileï¼š{profile}")

    while True:
        user_input = input("\nğŸ‘¤ User: ").strip()
        if user_input.lower() in ("q", "quit", "exit"):
            break

        reply = await _generate_response(user_input, profile)
        print(f"ğŸªª Agent: {reply}")

    print("Bye!")

if __name__ == "__main__":
    if "--local" in sys.argv:
        if sys.platform.startswith("win"):
            asyncio.set_event_loop_policy(
                asyncio.WindowsSelectorEventLoopPolicy()
            )
        asyncio.run(local_chat_loop())
    else:
        print("ğŸªª Eligibility Agent Server starting...", file=sys.stderr)
        mcp.run()
