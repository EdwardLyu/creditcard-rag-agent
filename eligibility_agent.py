# eligibility_agent.py
import os
import sys
import json
import asyncio
from pathlib import Path

from mcp.server.fastmcp import FastMCP
from openai import OpenAI
from rag_search import search_chunks
import logging   
from dotenv import load_dotenv

# === è®€å– Gemini è¨­å®šï¼ˆå–ä»£åŸæœ¬ Azure OpenAIï¼‰ ===
# 1. åˆå§‹åŒ–ç’°å¢ƒ
from pathlib import Path

# åœ¨é€™å€‹æª”æ¡ˆæ‰€åœ¨çš„è³‡æ–™å¤¾ï¼Œå¾€ä¸Šæ‰¾ .env
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

# === è®€å– Gemini è¨­å®šï¼ˆå–ä»£åŸæœ¬ Azure OpenAIï¼‰ ===
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = os.getenv(
    "GEMINI_BASE_URL",
    "https://generativelanguage.googleapis.com/v1beta/openai/"
)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")

try:
    llm_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url=GEMINI_BASE_URL,
    )
except Exception as e:
    print(f"âŒ Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
    llm_client = None
    
mcp = FastMCP("eligibility-agent")


# ==========================================
# 2. è¼•é‡ç‰ˆï¼šäº”å¼µå¡çš„ç”³è¾¦è¦å‰‡è¡¨
#   
# ==========================================

async def tool_check_eligibility(user_profile_json: str) -> dict:
    """
    ä½¿ç”¨ LLM + RAG å…§å®¹åˆ¤æ–· eligibilityã€‚
    âœ… ä¸å†å¼·åˆ¶ LLM è¼¸å‡º JSONï¼Œå› æ­¤ä¸æœƒå†å‡ºç¾ã€Œç„¡æ³•è§£æã€ã€‚
    """
    user = json.loads(user_profile_json)
    
    metadata = {
         "doc_type": "credit_card_profile"
    }
    
    # 1) å¾ RAG æœå°‹è©²å¡ç‰‡ç›¸é—œå…§å®¹
    try:
        rag_results = search_chunks(
            query="ä¿¡ç”¨å¡ç”³è¾¦è³‡æ ¼æ¢ä»¶",
            metadata_filter=metadata,
            top_k=20
        )
    except Exception as e:
        rag_results = []
    print(rag_results)
    # 2) è®“ LLM æ ¹æ“š RAG å…§å®¹åš eligibility æ¨è«–ï¼ˆç”¨è‡ªç„¶èªè¨€å³å¯ï¼‰
    prompt = f"""
ä½ æ˜¯ä¸€ä½ä¿¡ç”¨å¡ç”³è¾¦è³‡æ ¼åˆ†æå°ˆå®¶ã€‚è«‹ä½ æ ¹æ“šæ•¸å¼µä¿¡ç”¨å¡çš„ç”³è¾¦è³‡è¨Šåˆ¤æ–·ä½¿ç”¨

ä½¿ç”¨è€…è³‡æ–™ï¼ˆJSONï¼‰ï¼š
{json.dumps(user, ensure_ascii=False)}


ä»¥ä¸‹æ˜¯å¾ RAG æœå°‹åˆ°çš„å¡ç‰‡å…§å®¹ï¼ˆå¯èƒ½åŒ…å«å›é¥‹ã€å„ªæƒ ã€æ¢æ¬¾ã€è³‡æ ¼ç­‰ï¼‰ï¼š
{json.dumps(rag_results, ensure_ascii=False)}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”æ¯å¼µå¡ç‰‡ï¼š
1) ç”³è«‹äººçš„å¹´é½¡/æ”¶å…¥/å­¸ç”Ÿèº«åˆ†æ˜¯å¦æœ‰é”åˆ°æ˜ç¢ºé–€æª»ï¼Ÿï¼ˆè‹¥æ²’æœ‰å¯«å°±èªªã€Œè³‡æ–™ä¸è¶³ã€ï¼‰
2) ä»¥æ­¤ä½¿ç”¨è€…æ¢ä»¶ï¼Œçµ¦èˆ‡æ¯å¼µå¡ã€Œå»ºè­°ç”³è¾¦ / ä¸å»ºè­° / è³‡è¨Šä¸è¶³ã€å…¶ä¸€
3) 2ï½4 é»æ¢åˆ—ç†ç”±

ç›´æ¥è¼¸å‡ºæ–‡å­—ï¼Œä¸è¦ JSONã€‚
"""

    resp = llm_client.chat.completions.create(
        model=GEMINI_MODEL,
        messages=[{"role": "user", "content": prompt}],
    )

    explanation = (resp.choices[0].message.content or "").strip()

    # 3) æˆ‘å€‘è‡ªå·±åŒ…æˆ dict å›å»ï¼ˆé¿å…è§£æï¼‰
    return explanation 


# ==========================================
# 3. å·¥å…· Schema èˆ‡ System Prompt
# ==========================================

INTERNAL_TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "tool_check_eligibility",
            "description": "æ ¹æ“šä½¿ç”¨è€…æ¢ä»¶ï¼Œæª¢æŸ¥æŒ‡å®šä¿¡ç”¨å¡æ˜¯å¦ç¬¦åˆç”³è¾¦é–€æª»ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "user_profile_json": {
                        "type": "string",
                        "description": "ä½¿ç”¨è€…è³‡æ–™çš„ JSON å­—ä¸²ï¼Œä¾‹å¦‚ {\"age\":23,\"annual_income\":450000,\"is_student\":false}"
                    }
                },
                "required": ["user_profile_json"]
            }
        }
    }
]

ELIGIBILITY_SYSTEM_PROMPT = """
ä½ æ˜¯ã€Œä¿¡ç”¨å¡ç”³è¾¦è³‡æ ¼å°ˆå®¶ã€ï¼Œè² è²¬åˆ¤æ–·ä½¿ç”¨è€…æ˜¯å¦é©åˆç”³è¾¦ç‰¹å®šä¿¡ç”¨å¡ã€‚

### å·¥å…·ä½¿ç”¨è¦å‰‡
- ä»»ä½•éœ€è¦åˆ¤æ–·ã€Œå¯ä¸å¯ä»¥è¾¦ã€ã€ã€Œéä»¶æ©Ÿç‡é«˜ä¸é«˜ã€ã€ã€Œå“ªå¼µæ¯”è¼ƒå®¹æ˜“ç”³è¾¦ã€çš„å•é¡Œï¼Œ
  éƒ½å¿…é ˆå‘¼å« `tool_check_eligibility`ï¼Œå–å¾—æ¯å¼µå¡çš„æ©Ÿæ¢°å¼åˆ¤æ–·çµæœèˆ‡åŸå› ã€‚

### å›ç­”åŸå‰‡
1. **å…ˆçœ‹çµæ§‹åŒ–çµæœï¼Œå†è£œå……èªªæ˜**ï¼š
   - å…ˆä¾ç…§å·¥å…·å›å‚³çš„ statusï¼ˆâœ…/âŒï¼‰åšæ•´ç†ã€‚
   - å†ç”¨æ¢åˆ—å¼èªªæ˜ç†ç”±ï¼Œä¾‹å¦‚å¹´é½¡ã€å¹´æ”¶ã€å­¸ç”Ÿèº«åˆ†ç­‰ã€‚
2. **ä¸è¦äº‚çŒœéŠ€è¡Œå…§è¦**ï¼š
   - å·¥å…·æ²’æœ‰æä¾›çš„è³‡æ–™ï¼Œå°±èªªã€Œæ­¤éƒ¨åˆ†ä»ä»¥éŠ€è¡Œå¯¦éš›å¯©æ ¸ç‚ºæº–ã€ã€‚
3. **è¼¸å‡ºæ ¼å¼å»ºè­°**ï¼š
   - å…ˆçµ¦ä¸€å€‹ç¸½çµï¼šæ¯”å¦‚ã€Œæ•´é«”ä¾†èªªï¼Œä½ æœ€é©åˆ Aã€B å¡ã€ã€‚
   - å†åˆ—å‡ºæ¯å¼µå¡ï¼šå¡å / å»ºè­° / åŸå› ï¼ˆæ¢åˆ—ï¼‰ã€‚
4. **user_profile ä¾†æº**ï¼š
   - ä½ æœƒæ”¶åˆ°ä¸€å€‹ `user_profile` å­—ä¸²åƒæ•¸ï¼Œå¯ç›´æ¥ç•¶ä½œ JSONï¼Œ
     ä¹Ÿå¯ä»¥ä¾ä½¿ç”¨è€…åœ¨å°è©±ä¸­è£œå……çš„è³‡è¨Šåšå£é ­è§£é‡‹ã€‚

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
"""

# ==========================================
# 4. REACT LOOP
# ==========================================

async def _generate_response(user_query: str, user_profile: str = "") -> str:

    messages = [
        {"role": "system", "content": ELIGIBILITY_SYSTEM_PROMPT},
        {
            "role": "user",
            "content": (
                f"ä½¿ç”¨è€…è³‡æ–™(user_profile JSON)ï¼š{user_profile}\n"
                f"ä½¿ç”¨è€…å•é¡Œï¼š{user_query}"
            ) if user_profile else user_query
        },
    ]

    MAX_TURNS = 5
    turn = 0

    try:
        while turn < MAX_TURNS:
            turn += 1

            resp = llm_client.chat.completions.create(
                model=GEMINI_MODEL,
                messages=messages,
                tools=INTERNAL_TOOLS_SCHEMA,
                tool_choice="auto",
            )

            msg = resp.choices[0].message
            messages.append(msg)

            # æ²’æœ‰å†å‘¼å«å·¥å…· â†’ ç›´æ¥å›è¦†
            if not msg.tool_calls:
                return msg.content

            # åŸ·è¡Œå·¥å…·
            for tool_call in msg.tool_calls:
                fname = tool_call.function.name
                args = json.loads(tool_call.function.arguments)

                if fname == "tool_check_eligibility":
                    tool_result = await tool_check_eligibility(**args)
                else:
                    tool_result = json.dumps(
                        {"error": f"Unknown tool: {fname}"},
                        ensure_ascii=False
                    )

                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": fname,
                    "content": tool_result,
                })

        return "âš ï¸ è¶…éæ€è€ƒæ¬¡æ•¸ä¸Šé™ï¼Œç„¡æ³•å–å¾—å®Œæ•´è³‡è¨Šã€‚"

    except Exception as e:
        return f"âŒ Agent åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}"

# ==========================================
# 5. MCP Tool Entry
# ==========================================

@mcp.tool()
async def eligibility_agent(user_query: str, user_profile: str = "") -> str:
    """
    ä¸»è¦é€²å…¥é»ï¼šæª¢æŸ¥æŒ‡å®šå¡ç‰‡çš„ç”³è¾¦è³‡æ ¼ã€‚
    - user_query: ä½¿ç”¨è€…è‡ªç„¶èªè¨€å•é¡Œ
    - user_profile: å»ºè­°å‚³ JSON å­—ä¸²ï¼Œä¾‹å¦‚ {"age":23,"annual_income":450000,"is_student":false}
    """
    print(f"ğŸªª [Eligibility Agent] æ”¶åˆ°è«‹æ±‚ | Query={user_query}", file=sys.stderr)
    return await _generate_response(user_query, user_profile)


# ==========================================
# 6. Local æ¸¬è©¦æ¨¡å¼
# ==========================================

async def local_chat_loop():
    print("\nğŸªª --- Eligibility Agent Local Mode ---")
    print("è¼¸å…¥ 'q' é›¢é–‹")

    profile = input("è«‹è¼¸å…¥ user_profile JSON (å¯ç•™ç©º): ").strip()
    if not profile:
        # çµ¦ä¸€å€‹ç¤ºç¯„ç”¨ profile
        profile = json.dumps(
            {"age": 23, "annual_income": 450000, "is_student": False},
            ensure_ascii=False,
        )
        print(f"ğŸ‘‰ ä½¿ç”¨é è¨­ profileï¼š{profile}")

    while True:
        user_input = input("\nğŸ‘¤ User: ").strip()
        if user_input.lower() in ("q", "quit", "exit"):
            break

        reply = await _generate_response(user_input, profile)
        print(f"ğŸªª Agent: {reply}")

    print("Bye!")

if __name__ == "__main__":
    if "--local" in sys.argv:
        if sys.platform.startswith("win"):
            asyncio.set_event_loop_policy(
                asyncio.WindowsSelectorEventLoopPolicy()
            )
        asyncio.run(local_chat_loop())
    else:
        print("ğŸªª Eligibility Agent Server starting...", file=sys.stderr)
        mcp.run()
