# comparing_agent.py
import os
import sys
import json
import asyncio
from pathlib import Path

# 3rd party imports
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from openai import OpenAI

# === å°å…¥ä½ çš„ RAG æœå°‹æ¨¡çµ„ ===
# ç¢ºä¿ rag_search.py, llm_utils.py å’Œ cards_rag_embedded.jsonl åœ¨åŒä¸€ç›®éŒ„ä¸‹
try:
    from rag_search import search_chunks, load_index
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° rag_search.pyï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚", file=sys.stderr)
    sys.exit(1)

# ==========================================
# 1. åˆå§‹åŒ–ç’°å¢ƒèˆ‡è¨­å®š
# ==========================================

# è¼‰å…¥ .env
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = os.getenv(
    "GEMINI_BASE_URL",
    "https://generativelanguage.googleapis.com/v1beta/openai/"
)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp") 

# åˆå§‹åŒ– Gemini Client
try:
    llm_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url=GEMINI_BASE_URL,
    )
except Exception as e:
    print(f"âŒ Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
    llm_client = None

# é å…ˆè¼‰å…¥ RAG è³‡æ–™åº« (åŠ é€Ÿç¬¬ä¸€æ¬¡æœå°‹)
print("ğŸ“š æ­£åœ¨åˆå§‹åŒ– RAG çŸ¥è­˜åº« (è¼‰å…¥ jsonl èˆ‡ embedding æ¨¡å‹)...", file=sys.stderr)
try:
    # é€™æœƒè§¸ç™¼ llm_utils è¼‰å…¥ BGE-M3 æ¨¡å‹ï¼Œç¬¬ä¸€æ¬¡æœƒæ¯”è¼ƒä¹…
    load_index()
    print("âœ… RAG çŸ¥è­˜åº«è¼‰å…¥å®Œæˆï¼", file=sys.stderr)
except Exception as e:
    print(f"âŒ RAG è¼‰å…¥å¤±æ•—: {e}", file=sys.stderr)

mcp = FastMCP("comparing-expert-agent")

# ==========================================
# 2. å®šç¾©çœŸå¯¦å·¥å…· (Real Tools)
# ==========================================

async def tool_search_bank_info(query: str, card_filter: str = None) -> str:
    """
    æœå°‹éŠ€è¡Œç”¢å“ã€æ¬Šç›Šæˆ–ä¿¡ç”¨å¡ç›¸é—œè³‡è¨Šã€‚
    """
    print(f"    ğŸ” [RAG Search] æœå°‹: {query} | éæ¿¾å¡ç‰‡: {card_filter}", file=sys.stderr)
    
    # search_chunks å…§éƒ¨æœƒå‘¼å« llm_utils.query_ai_embedding (CPU å¯†é›†é‹ç®—)
    # ä½¿ç”¨ to_thread æŠŠå®ƒä¸Ÿåˆ°èƒŒæ™¯åŸ·è¡Œï¼Œé¿å…å¡ä½ async äº‹ä»¶è¿´åœˆ
    try:
        results = await asyncio.to_thread(
            search_chunks, 
            query=query, 
            card_filter=card_filter, 
            top_k=5  # å–å‰ 5 ç­†æœ€ç›¸é—œ
        )
        
        if not results:
            return json.dumps({"result": "æŸ¥ç„¡ç›¸é—œè³‡æ–™ï¼Œè«‹å˜—è©¦æ›å€‹é—œéµå­—ã€‚"})

        # æ•´ç†å›å‚³çµæœï¼Œç¯€çœ token ä¸¦è®“ LLM å¥½è®€
        simplified_results = []
        for r in results:
            simplified_results.append({
                "card": r.get("card_name", "æœªçŸ¥å¡ç‰‡"),
                "type": r.get("doc_type", "ä¸€èˆ¬è³‡è¨Š"),
                "content": r.get("text", "")
            })
            
        return json.dumps(simplified_results, ensure_ascii=False)

    except Exception as e:
        error_msg = f"æœå°‹åŸ·è¡ŒéŒ¯èª¤: {str(e)}"
        print(f"âŒ {error_msg}", file=sys.stderr)
        return json.dumps({"error": error_msg})

# ==========================================
# 3. å·¥å…· Schemas èˆ‡ System Prompt
# ==========================================

INTERNAL_TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "tool_search_bank_info",
            "description": "æœå°‹ä¿¡ç”¨å¡æ¬Šç›Šã€å›é¥‹è¦å‰‡ã€å¹´è²»ç­‰éŠ€è¡Œç”¢å“è³‡è¨Šã€‚ç•¶ä½¿ç”¨è€…è©¢å•å…·é«”å¡ç‰‡ç´°ç¯€æ™‚å¿…é ˆä½¿ç”¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœå°‹é—œéµå­—æˆ–å•é¡Œï¼Œä¾‹å¦‚ 'CUBEå¡æ—¥æœ¬å›é¥‹' æˆ– 'ä¸–ç•Œå¡å¹´è²»'"
                    },
                    "card_filter": {
                        "type": "string",
                        "description": "è‹¥å•é¡Œæ˜ç¢ºé‡å°æŸå¼µå¡ï¼Œå¯å¡«å…¥å¡ç‰‡åç¨±ä»¥éæ¿¾é›œè¨Š (å¦‚ 'CUBEå¡')"
                    }
                },
                "required": ["query"]
            }
        }
    }
]

COMPARING_SYSTEM_PROMPT = """
ä½ æ˜¯åœ‹æ³°ä¸–è¯éŠ€è¡Œçš„ã€Œè³‡æ·±ä¿¡ç”¨å¡ç”¢å“é¡§å•ã€ã€‚
ä½ çš„è³‡æ–™ä¾†æºæ˜¯å…§éƒ¨çš„ RAG çŸ¥è­˜åº«ï¼Œè«‹æ ¹æ“šæœå°‹çµæœä¾†å›ç­”ä½¿ç”¨è€…ã€‚

### å›ç­”åŸå‰‡ï¼š
1. **è­‰æ“šèªªè©±**ï¼šä½¿ç”¨è€…å•å…·é«”æ¬Šç›Šï¼ˆå¦‚å›é¥‹ç‡ã€å¹´è²»ã€è¦å‰‡ï¼‰æ™‚ï¼Œ**å¿…é ˆ**ä½¿ç”¨ `tool_search_bank_info` æŸ¥è©¢ã€‚
2. **èª å¯¦å‘ŠçŸ¥**ï¼šå¦‚æœæœå°‹çµæœæ²’æœ‰æåˆ°ï¼Œå°±èªªã€Œè³‡æ–™åº«ä¸­ç›®å‰æ²’æœ‰ç›¸é—œè³‡è¨Šã€ï¼Œä¸è¦æ†‘ç©ºæé€ ã€‚
3. **å‹å–„å°ˆæ¥­**ï¼šå›ç­”æ™‚è«‹æ•´ç†é‡é»ï¼ˆæ¢åˆ—å¼ï¼‰ï¼Œèªæ°£è¦ªåˆ‡ã€‚
4. **æ¯”è¼ƒæƒ…å¢ƒ**ï¼šè‹¥ä½¿ç”¨è€…è¦æ¯”è¼ƒå…©å¼µå¡ï¼ˆå¦‚ Aå¡ vs Bå¡ï¼‰ï¼Œè«‹åˆ†åˆ¥æœå°‹é€™å…©å¼µå¡çš„è³‡æ–™ï¼Œå†ç¶œåˆå›ç­”ã€‚

### æ€è€ƒæµç¨‹ï¼š
- æ”¶åˆ°å•é¡Œ -> åˆ¤æ–·é—œéµå­— -> å‘¼å«æœå°‹å·¥å…· -> é–±è®€æœå°‹çµæœ -> æ•´ç†ä¸¦å›ç­”ã€‚
"""

# ==========================================
# 4. REACT LOOP (æ ¸å¿ƒé‚è¼¯)
# ==========================================

async def _generate_response(user_query: str, user_profile: str = "") -> str:
    if not llm_client:
        return "âŒ ç³»çµ±éŒ¯èª¤ï¼šLLM client æœªåˆå§‹åŒ–"

    # å»ºæ§‹å°è©±æ­·å²
    messages = [
        {"role": "system", "content": COMPARING_SYSTEM_PROMPT},
        {"role": "user", "content": f"ä½¿ç”¨è€…èƒŒæ™¯ï¼š{user_profile}\nä½¿ç”¨è€…å•é¡Œï¼š{user_query}" if user_profile else user_query}
    ]

    MAX_TURNS = 5
    turn = 0

    try:
        while turn < MAX_TURNS:
            turn += 1

            # 1. å‘¼å« LLM
            response = llm_client.chat.completions.create(
                model=GEMINI_MODEL,
                messages=messages,
                tools=INTERNAL_TOOLS_SCHEMA,
                tool_choice="auto",
            )

            msg = response.choices[0].message
            messages.append(msg)

            # 2. è‹¥æ²’æœ‰è¦å‘¼å«å·¥å…·ï¼Œç›´æ¥å›å‚³ç­”æ¡ˆ
            if not msg.tool_calls:
                return msg.content

            # 3. åŸ·è¡Œå·¥å…·
            for tool_call in msg.tool_calls:
                fname = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                
                tool_result = ""
                if fname == "tool_search_bank_info":
                    tool_result = await tool_search_bank_info(**args)
                else:
                    tool_result = json.dumps({"error": "Unknown tool"})

                # å°‡å·¥å…·çµæœå›å‚³çµ¦ LLM
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": fname,
                    "content": tool_result
                })

        return "âš ï¸ è¶…éæ€è€ƒæ¬¡æ•¸ä¸Šé™ï¼Œç„¡æ³•å–å¾—å®Œæ•´è³‡è¨Šã€‚"

    except Exception as e:
        return f"âŒ Agent åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}"

# ==========================================
# 5. MCP Tool Entry
# ==========================================

@mcp.tool()
async def comparing_agent(user_query: str, user_profile: str = "") -> str:
    """ä¸»è¦é€²å…¥é»ï¼šæ¥æ”¶ä½¿ç”¨è€…å•é¡Œï¼Œå›å‚³æ¯”è¼ƒæˆ–æ¨è–¦çµæœ"""
    print(f"âš–ï¸ [Comparing Agent] æ”¶åˆ°è«‹æ±‚ | Query={user_query}", file=sys.stderr)
    return await _generate_response(user_query, user_profile)

# ==========================================
# Local æ¸¬è©¦ Loop
# ==========================================

async def local_chat_loop():
    print("\nâš–ï¸ --- Comparing Agent Local Mode (RAG Enabled) ---")
    print("è¼¸å…¥ 'q' é›¢é–‹")
    
    # æ¸¬è©¦ç’°å¢ƒæª¢æŸ¥
    if not os.path.exists("cards_rag_embedded.jsonl"):
        print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° cards_rag_embedded.jsonlï¼Œæœå°‹åŠŸèƒ½å°‡å¤±æ•ˆã€‚")

    profile = input("è¨­å®š user_profile (å¯ç•™ç©º): ").strip()

    while True:
        user_input = input("\nğŸ‘¤ User: ").strip()
        if user_input.lower() in ("q", "quit", "exit"):
            break
        
        reply = await _generate_response(user_input, profile)
        print(f"âš–ï¸ Agent: {reply}")

    print("Bye!")

if __name__ == "__main__":
    if "--local" in sys.argv:
        if sys.platform.startswith("win"):
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(local_chat_loop())
    else:
        print("âš–ï¸ Comparing Agent Server starting...", file=sys.stderr)
        mcp.run()