# comparing_agent.py
import os
import sys
import json
import asyncio
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from openai import OpenAI  # âœ… æ”¹æˆä½¿ç”¨ OpenAI clientï¼ˆæŒ‡å‘ Gemini APIï¼‰

# 1. åˆå§‹åŒ–ç’°å¢ƒ
from pathlib import Path

# åœ¨é€™å€‹æª”æ¡ˆæ‰€åœ¨çš„è³‡æ–™å¤¾ï¼Œå¾€ä¸Šæ‰¾ .env
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

# === è®€å– Gemini è¨­å®š ===
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = os.getenv(
    "GEMINI_BASE_URL",
    "https://generativelanguage.googleapis.com/v1beta/openai/"
)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")

# å»ºç«‹ Gemini-compatible client
try:
    llm_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url=GEMINI_BASE_URL,
    )
except Exception as e:
    print(f"âŒ Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
    llm_client = None

mcp = FastMCP("comparing-expert-agent")

# ==========================================
# 2. å…§éƒ¨å·¥å…· (Internal Tools)
# ==========================================

async def tool_example_1(card_name: str) -> str:
    print(f"   âš™ï¸ [Internal Tool] æŸ¥å›é¥‹ | card={card_name}", file=sys.stderr)
    if "CUBE" in card_name.upper():
        return json.dumps({"card": "CUBEå¡", "reward_rate": "3%", "note": "éœ€åˆ‡æ›æ¬Šç›Š"})
    elif "ROSE" in card_name.upper():
        return json.dumps({"card": "Rose Givingå¡", "reward_rate": "3%", "note": "ç¯€å‡æ—¥é™å®š"})
    else:
        return json.dumps({"error": "æŸ¥ç„¡æ­¤å¡è³‡æ–™"})

async def tool_example_2(score_a: int, score_b: int) -> str:
    print(f"   âš™ï¸ [Internal Tool] æ¯”åˆ†æ•¸ | {score_a} vs {score_b}", file=sys.stderr)
    diff = score_a - score_b
    if diff > 0:
        return f"Aæ¯”Bé«˜ {diff} åˆ†"
    elif diff < 0:
        return f"Bæ¯”Aé«˜ {abs(diff)} åˆ†"
    else:
        return "å…©è€…åˆ†æ•¸ç›¸åŒ"

async def tool_example_3(user_type: str) -> str:
    print(f"   âš™ï¸ [Internal Tool] æ¨è–¦å¡ç‰‡ | ä½¿ç”¨è€…={user_type}", file=sys.stderr)
    if "å­¸ç”Ÿ" in user_type:
        return "æ¨è–¦: CUBEå¡ (é–€æª»ä½)"
    elif "å¯Œè±ª" in user_type:
        return "æ¨è–¦: ä¸–ç•Œå¡ (æ¬Šç›Šå¤š)"
    else:
        return "æ¨è–¦: ç¾é‡‘å›é¥‹å¾¡ç’½å¡ (é€šç”¨)"

# ==========================================
# 3. å·¥å…· Schemas
# ==========================================

INTERNAL_TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "tool_example_1",
            "description": "æŸ¥è©¢å¡ç‰‡åŸºç¤å›é¥‹ç‡ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "card_name": {"type": "string"}
                },
                "required": ["card_name"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "tool_example_2",
            "description": "æ¯”è¼ƒå…©å€‹åˆ†æ•¸å·®ç•°ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "score_a": {"type": "integer"},
                    "score_b": {"type": "integer"}
                },
                "required": ["score_a", "score_b"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "tool_example_3",
            "description": "ä¾ä½¿ç”¨è€…èº«åˆ†æ¨è–¦å¡ç‰‡ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "user_type": {"type": "string"}
                },
                "required": ["user_type"]
            }
        }
    }
]

COMPARING_SYSTEM_PROMPT = """
ä½ æ˜¯åœ‹æ³°ä¸–è¯éŠ€è¡Œçš„ã€Œä¿¡ç”¨å¡æ¯”è¼ƒèˆ‡æ¨è–¦é¡§å•ã€ã€‚
ä½ çš„ä»»å‹™æ˜¯å›ç­”ä½¿ç”¨è€…çš„æ¯”è¼ƒå•é¡Œæˆ–æ¨è–¦è«‹æ±‚ã€‚

å¯ç”¨å·¥å…·ï¼š
- tool_example_1ï¼šæŸ¥å›é¥‹ç‡
- tool_example_2ï¼šæ¯”åˆ†æ•¸
- tool_example_3ï¼šä¾èº«åˆ†æ¨è–¦å¡ç‰‡

åŸå‰‡ï¼š
- å„ªå…ˆä½¿ç”¨å·¥å…·ä¾†ç²å¾—è³‡æ–™
- çµæœéœ€æ•´ç†æˆæ¸…æ¥šã€è¦ªåˆ‡çš„å»ºè­°
"""

# ==========================================
# 4. REACT LOOPï¼ˆæ ¸å¿ƒé‚è¼¯ï¼‰
# ==========================================

async def _generate_response(user_query: str, user_profile: str = "") -> str:
    if not llm_client:
        return "âŒ ç³»çµ±éŒ¯èª¤ï¼šLLM client æœªåˆå§‹åŒ–"

    # å°‡èƒŒæ™¯è³‡æ–™ä¸€èµ·åŠ å…¥ prompt
    full_content = f"ä½¿ç”¨è€…å•é¡Œï¼š{user_query}"
    if user_profile:
        full_content += f"\nä½¿ç”¨è€…èƒŒæ™¯ï¼š{user_profile}"

    messages = [
        {"role": "system", "content": COMPARING_SYSTEM_PROMPT},
        {"role": "user", "content": full_content}
    ]

    MAX_TURNS = 5
    turn = 0

    try:
        while turn < MAX_TURNS:
            turn += 1

            # === å‘¼å« Gemini ===
            response = llm_client.chat.completions.create(
                model=GEMINI_MODEL,
                messages=messages,
                tools=INTERNAL_TOOLS_SCHEMA,
                tool_choice="auto",
            )

            msg = response.choices[0].message
            messages.append(msg)

            # è‹¥æ¨¡å‹ç›´æ¥çµ¦ç­”æ¡ˆ â†’ çµæŸ
            if not msg.tool_calls:
                return msg.content

            # === åŸ·è¡Œå·¥å…· ===
            for tool_call in msg.tool_calls:
                fname = tool_call.function.name
                args = json.loads(tool_call.function.arguments)

                if fname == "tool_example_1":
                    result = await tool_example_1(**args)
                elif fname == "tool_example_2":
                    result = await tool_example_2(**args)
                elif fname == "tool_example_3":
                    result = await tool_example_3(**args)
                else:
                    result = json.dumps({"error": "Unknown internal tool"})

                # å›å‚³çµ¦ LLM
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": fname,
                    "content": result
                })

        return "âš ï¸ æ€è€ƒæ¬¡æ•¸éå¤šï¼ˆè¶…é MAX_TURNSï¼‰ï¼Œæœªèƒ½å®Œæˆå›ç­”ã€‚"

    except Exception as e:
        return f"âŒ Agent åŸ·è¡ŒéŒ¯èª¤ï¼š{e}"

# ==========================================
# 5. MCP Tool Entry
# ==========================================

@mcp.tool()
async def comparing_agent(user_query: str, user_profile: str = "") -> str:
    print(f"âš–ï¸ [Comparing Agent] æ”¶åˆ°è«‹æ±‚ | Query={user_query}", file=sys.stderr)
    return await _generate_response(user_query, user_profile)

# ==========================================
# Local æ¸¬è©¦
# ==========================================

async def local_chat_loop():
    print("\nâš–ï¸ --- Comparing Agent Local Mode ---")
    print("è¼¸å…¥ 'q' é›¢é–‹")

    profile = input("è¨­å®š user_profile (å¯ç•™ç©º): ").strip()

    while True:
        user_input = input("\nğŸ‘¤ User: ").strip()
        if user_input.lower() in ("q", "quit", "exit"):
            break

        reply = await _generate_response(user_input, profile)
        print(f"âš–ï¸ Agent: {reply}")

    print("Bye!")

# ==========================================
# ä¼ºæœå™¨å…¥å£
# ==========================================

if __name__ == "__main__":
    if "--local" in sys.argv:
        if sys.platform.startswith("win"):
            asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(local_chat_loop())
    else:
        print("âš–ï¸ Comparing Agent Server starting...", file=sys.stderr)
        mcp.run()
