import os
import sys
import json
import asyncio
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from openai import AzureOpenAI

# 1. åˆå§‹åŒ–ç’°å¢ƒ
load_dotenv()

try:
    aoai_client = AzureOpenAI(
        api_key=os.getenv("AOAI_KEY"),
        azure_endpoint=os.getenv("AOAI_URL"),
        api_version=os.getenv("AOAI_MODEL_VERSION"),
    )
except Exception as e:
    print(f"âŒ AOAI Client åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
    aoai_client = None

mcp = FastMCP("comparing-expert-agent")

# ==========================================
# 2. å®šç¾©å…§éƒ¨å·¥å…· (Internal Tools) - å»šæˆ¿è£¡çš„å·¥å…·
# ==========================================
# é€™äº›å·¥å…·åªåœ¨ Server å…§éƒ¨é‹ä½œï¼ŒClient ç«¯(rag_mcp_client.py) å®Œå…¨ä¸çŸ¥é“å®ƒå€‘çš„å­˜åœ¨

async def tool_example_1(card_name: str) -> str:
    """æ¨¡æ“¬æŸ¥è©¢è³‡æ–™åº«ï¼šå–å¾—å¡ç‰‡åŸºç¤å›é¥‹ç‡"""
    print(f"   âš™ï¸ [Internal Tool] åŸ·è¡Œ tool_example_1 (æŸ¥å›é¥‹) | åƒæ•¸: {card_name}", file=sys.stderr)
    # æ¨¡æ“¬è³‡æ–™åº«å›å‚³
    if "CUBE" in card_name.upper():
        return json.dumps({"card": "CUBEå¡", "reward_rate": "3%", "note": "éœ€åˆ‡æ›æ¬Šç›Š"})
    elif "ROSE" in card_name.upper():
        return json.dumps({"card": "Rose Givingå¡", "reward_rate": "3%", "note": "ç¯€å‡æ—¥é™å®š"})
    else:
        return json.dumps({"error": "æŸ¥ç„¡æ­¤å¡ç‰‡è³‡æ–™"})

async def tool_example_2(score_a: int, score_b: int) -> str:
    """æ¨¡æ“¬è¨ˆç®—å·¥å…·ï¼šæ¯”è¼ƒå…©å€‹åˆ†æ•¸çš„å·®è·"""
    print(f"   âš™ï¸ [Internal Tool] åŸ·è¡Œ tool_example_2 (æ¯”åˆ†æ•¸) | åƒæ•¸: {score_a} vs {score_b}", file=sys.stderr)
    diff = score_a - score_b
    if diff > 0:
        return f"Aæ¯”Bé«˜ {diff} åˆ†"
    elif diff < 0:
        return f"Bæ¯”Aé«˜ {abs(diff)} åˆ†"
    else:
        return "å…©è€…åˆ†æ•¸ç›¸åŒ"

async def tool_example_3(user_type: str) -> str:
    """æ¨¡æ“¬æ¨è–¦ç³»çµ±ï¼šæ ¹æ“šä½¿ç”¨è€…é¡å‹æ¨è–¦å¡ç‰‡"""
    print(f"   âš™ï¸ [Internal Tool] åŸ·è¡Œ tool_example_3 (æ‰¾æ¨è–¦) | åƒæ•¸: {user_type}", file=sys.stderr)
    if "å­¸ç”Ÿ" in user_type:
        return "æ¨è–¦: CUBEå¡ (é–€æª»ä½)"
    elif "å¯Œè±ª" in user_type:
        return "æ¨è–¦: ä¸–ç•Œå¡ (æ¬Šç›Šå¤š)"
    else:
        return "æ¨è–¦: ç¾é‡‘å›é¥‹å¾¡ç’½å¡ (é€šç”¨)"

# ==========================================
# 3. å®šç¾©å·¥å…·æ¸…å–® (JSON Schema) - çµ¦ LLM çœ‹çš„èœå–®
# ==========================================
INTERNAL_TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "tool_example_1",
            "description": "æŸ¥è©¢ç‰¹å®šä¿¡ç”¨å¡çš„åŸºç¤å›é¥‹ç‡è³‡æ–™ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "card_name": {"type": "string", "description": "å¡ç‰‡åç¨±"}
                },
                "required": ["card_name"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "tool_example_2",
            "description": "æ¯”è¼ƒå…©å€‹æ•¸å€¼æˆ–æ¬Šç›Šåˆ†æ•¸çš„å·®ç•°ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "score_a": {"type": "integer", "description": "ç¬¬ä¸€å¼µå¡çš„åˆ†æ•¸"},
                    "score_b": {"type": "integer", "description": "ç¬¬äºŒå¼µå¡çš„åˆ†æ•¸"}
                },
                "required": ["score_a", "score_b"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "tool_example_3",
            "description": "æ ¹æ“šä½¿ç”¨è€…èº«åˆ†(å¦‚å­¸ç”Ÿã€å¯Œè±ª)ç²å–ç³»çµ±æ¨è–¦çš„å¡ç‰‡ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "user_type": {"type": "string", "description": "ä½¿ç”¨è€…é¡å‹"}
                },
                "required": ["user_type"]
            }
        }
    }
]

COMPARING_SYSTEM_PROMPT = """
ä½ æ˜¯åœ‹æ³°ä¸–è¯éŠ€è¡Œçš„ã€Œä¿¡ç”¨å¡æ¯”è¼ƒèˆ‡æ¨è–¦é¡§å•ã€ã€‚
ä½ çš„ä»»å‹™æ˜¯å›ç­”ä½¿ç”¨è€…çš„æ¯”è¼ƒå•é¡Œæˆ–æ¨è–¦è«‹æ±‚ã€‚

# å¯ç”¨å·¥å…·ï¼š
- `tool_example_1`: æŸ¥è©¢å¡ç‰‡å›é¥‹ç‡ã€‚
- `tool_example_2`: æ¯”è¼ƒå…©å€‹åˆ†æ•¸å·®ç•°ã€‚
- `tool_example_3`: æ ¹æ“šèº«åˆ†æ¨è–¦å¡ç‰‡ã€‚

# è¦å‰‡ï¼š
- ç›¡é‡ä½¿ç”¨å·¥å…·ä¾†ç²å–ç¢ºåˆ‡è³‡è¨Šï¼Œè€Œä¸æ˜¯æ†‘ç©ºçŒœæ¸¬ã€‚
- æ”¶åˆ°å·¥å…·çµæœå¾Œï¼Œè«‹æ•´åˆæˆè¦ªåˆ‡çš„é¡§å•å£å»å›è¦†ä½¿ç”¨è€…ã€‚
"""

# ==========================================
# 4. æ ¸å¿ƒé‚è¼¯å±¤ (ReAct Loop)
# ==========================================
async def _generate_response(user_query: str, user_profile: str = "") -> str:
    if not aoai_client:
        return "âŒ ç³»çµ±éŒ¯èª¤ï¼šAgent è…¦éƒ¨é€£ç·šå¤±æ•—ã€‚"

    # æº–å‚™åˆå§‹å°è©±æ­·å²
    full_content = f"ä½¿ç”¨è€…å•é¡Œï¼š{user_query}"
    if user_profile:
        full_content += f"\nä½¿ç”¨è€…èƒŒæ™¯ï¼š{user_profile}"

    messages = [
        {"role": "system", "content": COMPARING_SYSTEM_PROMPT},
        {"role": "user", "content": full_content}
    ]

    # è¨­å®šæœ€å¤§æ€è€ƒæ¬¡æ•¸ (é¿å…ç„¡çª®è¿´åœˆ)
    MAX_TURNS = 5
    current_turn = 0

    try:
        while current_turn < MAX_TURNS:
            current_turn += 1
            
            # 1. å‘¼å« LLM (æ€è€ƒ)
            response = aoai_client.chat.completions.create(
                model=os.getenv("AOAI_MODEL_VERSION"),
                messages=messages,
                tools=INTERNAL_TOOLS_SCHEMA, # çµ¦å®ƒçœ‹å…§éƒ¨å·¥å…·
                tool_choice="auto"
            )
            msg = response.choices[0].message
            messages.append(msg) # å°‡ LLM çš„å›æ‡‰åŠ å…¥æ­·å²

            # 2. åˆ¤æ–·æ˜¯å¦éœ€è¦å‘¼å«å·¥å…·
            if not msg.tool_calls:
                # LLM èªç‚ºä¸éœ€è¦å‘¼å«å·¥å…·ï¼Œç›´æ¥ç”Ÿæˆäº†å›ç­” -> ä»»å‹™çµæŸ
                return msg.content

            # 3. åŸ·è¡Œå·¥å…· (è¡Œå‹•)
            for tool_call in msg.tool_calls:
                func_name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                
                result_content = ""
                
                # ç°¡å–®çš„å·¥å…·è·¯ç”±
                if func_name == "tool_example_1":
                    result_content = await tool_example_1(**args)
                elif func_name == "tool_example_2":
                    result_content = await tool_example_2(**args)
                elif func_name == "tool_example_3":
                    result_content = await tool_example_3(**args)
                else:
                    result_content = json.dumps({"error": "Unknown tool"})

                # 4. å°‡å·¥å…·çµæœåŠ å…¥æ­·å² (è§€å¯Ÿ)
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": func_name,
                    "content": str(result_content)
                })
            
            # è¿´åœˆç¹¼çºŒï¼ŒLLM æœƒåœ¨ä¸‹ä¸€è¼ªçœ‹åˆ°å·¥å…·çµæœä¸¦é€²è¡Œæ•´åˆ...

        return "æ€è€ƒæ¬¡æ•¸éå¤šï¼Œç„¡æ³•ç”¢ç”Ÿå®Œæ•´å›ç­”ã€‚"

    except Exception as e:
        return f"Agent åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {str(e)}"

# ==========================================
# MCP ä»‹é¢å±¤
# ==========================================
@mcp.tool()
async def comparing_agent(user_query: str, user_profile: str = "") -> str:
    """ã€æ¯”è¼ƒèˆ‡æ¨è–¦å°ˆå®¶å…¥å£ã€‘æ¥æ”¶ä½¿ç”¨è€…å•é¡Œèˆ‡èƒŒæ™¯ï¼Œé€é LLM èˆ‡å…§éƒ¨å·¥å…·ç”Ÿæˆå»ºè­°ã€‚"""
    print(f"âš–ï¸ [Comparing Agent] æ”¶åˆ°è«‹æ±‚ (MCP) | Query: {user_query}", file=sys.stderr)
    return await _generate_response(user_query, user_profile)

# ==========================================
# Local æ¸¬è©¦å±¤
# ==========================================
async def local_chat_loop():
    print("\nâš–ï¸ --- æ¯”è¼ƒèˆ‡æ¨è–¦ Agent (æœ¬åœ°æ¸¬è©¦æ¨¡å¼) ---")
    print("è¼¸å…¥ 'q' é›¢é–‹ã€‚")
    print("(æ¸¬è©¦æç¤ºï¼šè©¦è‘—å• 'CUBEå¡å›é¥‹å¤šå°‘?' æˆ– 'æˆ‘æ˜¯å­¸ç”Ÿæ¨è–¦å“ªå¼µ?' æˆ– 'æ¯”è¼ƒ 100 å’Œ 80')")
    
    profile = input("è¨­å®šæ¸¬è©¦ç”¨ User Profile (æŒ‰ Enter è·³é): ").strip()
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ (User): ").strip()
            if user_input.lower() in ['q', 'quit', 'exit']:
                break
            if not user_input:
                continue
            
            print("âš–ï¸ (Agent): æ€è€ƒä¸­...", end="\r")
            reply = await _generate_response(user_input, profile)
            print(f"âš–ï¸ (Agent): {reply}")
            
        except KeyboardInterrupt:
            break
    print("\nBye!")

# ==========================================
# ä¸»ç¨‹å¼å…¥å£
# ==========================================
if __name__ == "__main__":
    if "--local" in sys.argv:
        if sys.platform.startswith('win'):
             asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(local_chat_loop())
    else:
        print("âš–ï¸ Comparing Agent Server starting...", file=sys.stderr)
        mcp.run()