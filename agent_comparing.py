# agent_comparing.py
import os
import sys
import json
import asyncio
from pathlib import Path

# 3rd party imports
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from openai import OpenAI

# === [é‡è¦] å°å…¥ä½ çš„ RAG æœå°‹å·¥å…· ===
# ç¢ºä¿ rag_search.py, llm_utils.py å’Œ cards_rag_embedded.jsonl åœ¨åŒä¸€ç›®éŒ„ä¸‹
try:
    from rag_search import search_chunks, load_index
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° rag_search.pyï¼Œè«‹ç¢ºèªæª”æ¡ˆä½ç½®ã€‚", file=sys.stderr)
    sys.exit(1)

# ==========================================
# 1. åˆå§‹åŒ–ç’°å¢ƒèˆ‡è¨­å®š
# ==========================================

# è¼‰å…¥ .env
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = os.getenv(
    "GEMINI_BASE_URL",
    "https://generativelanguage.googleapis.com/v1beta/openai/"
)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp") 

# åˆå§‹åŒ– Gemini Client
try:
    llm_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url=GEMINI_BASE_URL,
    )
except Exception as e:
    print(f"âŒ Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
    llm_client = None

# [é‡è¦] é å…ˆè¼‰å…¥ RAG è³‡æ–™åº«
# é€™æœƒè§¸ç™¼ llm_utils è¼‰å…¥ Embedding æ¨¡å‹ (BGE-M3)ï¼Œç¢ºä¿å¾ŒçºŒæœå°‹é€Ÿåº¦
print("ğŸ“š æ­£åœ¨åˆå§‹åŒ– RAG çŸ¥è­˜åº«...", file=sys.stderr)
try:
    load_index()
    print("âœ… RAG çŸ¥è­˜åº«è¼‰å…¥å®Œæˆï¼", file=sys.stderr)
except Exception as e:
    print(f"âŒ RAG è¼‰å…¥å¤±æ•—: {e}", file=sys.stderr)

# å»ºç«‹ MCP Server
mcp = FastMCP("comparing-expert-agent")

# ==========================================
# 2. å®šç¾©çœŸå¯¦å·¥å…· (Real Tools)
# ==========================================

async def tool_search_bank_info(query: str, card_filter: str = None) -> str:
    """
    æœå°‹éŠ€è¡Œç”¢å“ã€æ¬Šç›Šæˆ–ä¿¡ç”¨å¡ç›¸é—œè³‡è¨Šã€‚
    é€™æ˜¯ Agent å”¯ä¸€ç²å–å¤–éƒ¨çŸ¥è­˜çš„ç®¡é“ã€‚
    """
    print(f"    ğŸ” [RAG Search] æœå°‹: {query} | éæ¿¾å¡ç‰‡: {card_filter}", file=sys.stderr)
    
    # [é—œéµå„ªåŒ–]
    # search_chunks å…§éƒ¨æœƒåŸ·è¡Œ Embedding é‹ç®— (CPU/GPU å¯†é›†)
    # å¿…é ˆä½¿ç”¨ asyncio.to_thread æ”¾åˆ°èƒŒæ™¯åŸ·è¡Œï¼Œå¦å‰‡æœƒå¡æ­»æ•´å€‹ Agent
    try:
        results = await asyncio.to_thread(
            search_chunks, 
            query=query, 
            card_filter=card_filter, 
            top_k=5  # å–å‰ 5 ç­†æœ€ç›¸é—œ
        )
        
        if not results:
            return json.dumps({"result": "æŸ¥ç„¡ç›¸é—œè³‡æ–™ï¼Œè«‹å˜—è©¦æ›´æ›é—œéµå­—ã€‚"})

        # æ•´ç†å›å‚³çµæœ (ç²¾ç°¡åŒ–ä»¥ç¯€çœ Token)
        simplified_results = []
        for r in results:
            simplified_results.append({
                "card": r.get("card_name", "æœªçŸ¥å¡ç‰‡"),
                "type": r.get("doc_type", "ä¸€èˆ¬è³‡è¨Š"),
                "content": r.get("text", "")
            })
            
        return json.dumps(simplified_results, ensure_ascii=False)

    except Exception as e:
        error_msg = f"æœå°‹åŸ·è¡ŒéŒ¯èª¤: {str(e)}"
        print(f"âŒ {error_msg}", file=sys.stderr)
        return json.dumps({"error": error_msg})

# ==========================================
# 3. å·¥å…· Schemas èˆ‡ System Prompt
# ==========================================

INTERNAL_TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "tool_search_bank_info",
            "description": "æœå°‹ä¿¡ç”¨å¡æ¬Šç›Šã€å›é¥‹è¦å‰‡ã€å¹´è²»ç­‰è³‡è¨Šã€‚å›ç­”ä½¿ç”¨è€…é—œæ–¼ç”¢å“çš„å…·é«”å•é¡Œæ™‚å¿…é ˆä½¿ç”¨ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœå°‹é—œéµå­—ï¼Œä¾‹å¦‚ 'CUBEå¡æ—¥æœ¬å›é¥‹' æˆ– 'ä¸–ç•Œå¡å¹´è²»'"
                    },
                    "card_filter": {
                        "type": "string",
                        "description": "è‹¥å•é¡Œæ˜ç¢ºé‡å°æŸå¼µå¡ï¼Œå¯å¡«å…¥å¡ç‰‡åç¨±ä»¥ç²¾æº–éæ¿¾ (å¦‚ 'CUBEå¡')"
                    }
                },
                "required": ["query"]
            }
        }
    }
]

COMPARING_SYSTEM_PROMPT = """
ä½ æ˜¯åœ‹æ³°ä¸–è¯éŠ€è¡Œçš„ã€Œè³‡æ·±ä¿¡ç”¨å¡ç”¢å“é¡§å•ã€ã€‚
ä½ çš„çŸ¥è­˜ä¾†æºæ˜¯å…§éƒ¨çš„ RAG è³‡æ–™åº«ï¼Œé™¤æ­¤ä¹‹å¤–ä½ ä¸çŸ¥é“å…¶ä»–å³æ™‚è³‡è¨Šã€‚

### å›ç­”åŸå‰‡ï¼š
1. **ä¾æ“šäº‹å¯¦**ï¼šç•¶ä½¿ç”¨è€…è©¢å•æ¬Šç›Šã€æ•¸å­—ã€è¦å‰‡æ™‚ï¼Œ**å¿…é ˆ**ä½¿ç”¨ `tool_search_bank_info` æŸ¥è©¢ã€‚
2. **èª å¯¦å‘ŠçŸ¥**ï¼šå¦‚æœæœå°‹çµæœä¸­æ²’æœ‰è³‡æ–™ï¼Œè«‹ç›´æ¥èªªã€Œè³‡æ–™åº«ä¸­ç›®å‰æ²’æœ‰ç›¸é—œè³‡è¨Šã€ï¼Œä¸è¦ç·¨é€ ã€‚
3. **çµæ§‹åŒ–å›ç­”**ï¼šè«‹æ¶ˆåŒ–æœå°‹åˆ°çš„å…§å®¹ï¼Œç”¨æ¢åˆ—å¼æˆ–è¡¨æ ¼æ•´ç†çµ¦ä½¿ç”¨è€…ï¼Œä¸è¦åªè²¼åŸæ–‡ã€‚
4. **æ¯”è¼ƒæƒ…å¢ƒ**ï¼šè‹¥ä½¿ç”¨è€…å•ã€ŒAå¡è·ŸBå¡å“ªå€‹å¥½ï¼Ÿã€ï¼Œè«‹åˆ†åˆ¥æœå°‹å…©å¼µå¡çš„è³‡æ–™ï¼Œå†é€²è¡Œç¶œåˆæ¯”è¼ƒã€‚

### æ€è€ƒæµç¨‹ï¼š
- æ”¶åˆ°å•é¡Œ -> åˆ†æé—œéµå­— -> å‘¼å«æœå°‹å·¥å…· -> é–±è®€çµæœ -> æ•´ç†ä¸¦å›ç­”ã€‚
"""

# ==========================================
# 4. REACT LOOP (æ ¸å¿ƒé‚è¼¯)
# ==========================================

async def _generate_response(user_query: str, user_profile: str = "") -> str:
    if not llm_client:
        return "âŒ ç³»çµ±éŒ¯èª¤ï¼šLLM client æœªåˆå§‹åŒ–"

    # å»ºæ§‹å°è©±æ­·å²
    full_query = user_query
    if user_profile:
        full_query = f"ä½¿ç”¨è€…èƒŒæ™¯ï¼š{user_profile}\nä½¿ç”¨è€…å•é¡Œï¼š{user_query}"

    messages = [
        {"role": "system", "content": COMPARING_SYSTEM_PROMPT},
        {"role": "user", "content": full_query}
    ]

    MAX_TURNS = 5
    turn = 0

    try:
        while turn < MAX_TURNS:
            turn += 1

            # 1. å‘¼å« LLM
            response = llm_client.chat.completions.create(
                model=GEMINI_MODEL,
                messages=messages,
                tools=INTERNAL_TOOLS_SCHEMA,
                tool_choice="auto",
            )

            msg = response.choices[0].message
            messages.append(msg)

            # 2. è‹¥æ²’æœ‰è¦å‘¼å«å·¥å…·ï¼Œç›´æ¥å›å‚³ç­”æ¡ˆ
            if not msg.tool_calls:
                return msg.content

            # 3. åŸ·è¡Œå·¥å…·
            for tool_call in msg.tool_calls:
                fname = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                
                tool_result = ""
                if fname == "tool_search_bank_info":
                    tool_result = await tool_search_bank_info(**args)
                else:
                    tool_result = json.dumps({"error": "Unknown tool"})

                # å°‡å·¥å…·çµæœå›å‚³çµ¦ LLM
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": fname,
                    "content": tool_result
                })

        return "âš ï¸ æŠ±æ­‰ï¼Œæˆ‘æ€è€ƒå¤ªä¹…äº†ï¼Œç„¡æ³•æä¾›å®Œæ•´ç­”æ¡ˆã€‚"

    except Exception as e:
        return f"âŒ Agent åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {e}"

# ==========================================
# 5. MCP Tool Entry & Local Test
# ==========================================

@mcp.tool()
async def comparing_agent(user_query: str, user_profile: str = "") -> str:
    """ä¸»è¦é€²å…¥é»ï¼šæ¥æ”¶ä½¿ç”¨è€…å•é¡Œï¼Œå›å‚³æ¯”è¼ƒæˆ–æ¨è–¦çµæœ"""
    print(f"âš–ï¸ [Comparing Agent] æ”¶åˆ°è«‹æ±‚ | Query={user_query}", file=sys.stderr)
    return await _generate_response(user_query, user_profile)

async def local_chat_loop():
    print("\nâš–ï¸ --- Comparing Agent Local Mode (RAG Enabled) ---")
    print("æç¤ºï¼šè¼¸å…¥ 'q' é›¢é–‹")
    
    if not os.path.exists("cards_rag_embedded.jsonl"):
        print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° cards_rag_embedded.jsonlï¼Œæœå°‹åŠŸèƒ½å°‡å¤±æ•ˆã€‚")

    profile = input("è¨­å®š user_profile (ä¾‹å¦‚ 'å­¸ç”Ÿ', 'å¸¸å‡ºåœ‹', å¯ç•™ç©º): ").strip()

    while True:
        user_input = input("\nğŸ‘¤ User: ").strip()
        if user_input.lower() in ("q", "quit", "exit"):
            break
        
        reply = await _generate_response(user_input, profile)
        print(f"âš–ï¸ Agent: {reply}")

    print("Bye!")

if __name__ == "__main__":
    # æ”¯æ´ Windows çš„ asyncio loop ç­–ç•¥
    if sys.platform.startswith("win"):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

    if "--local" in sys.argv:
        asyncio.run(local_chat_loop())
    else:
        print("âš–ï¸ Comparing Agent Server starting...", file=sys.stderr)
        mcp.run()