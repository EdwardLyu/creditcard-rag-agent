import os
import sys
import json
import asyncio
from rag_search import search_chunks
from mcp.server.fastmcp import FastMCP
from dotenv import load_dotenv
from openai import OpenAI  # âœ… æ”¹æˆä½¿ç”¨ OpenAI clientï¼ˆæŒ‡å‘ Gemini ç›¸å®¹ç«¯é»ï¼‰
from rag_search import search_chunks 

# 1. åˆå§‹åŒ–ç’°å¢ƒ
from pathlib import Path

# åœ¨é€™å€‹æª”æ¡ˆæ‰€åœ¨çš„è³‡æ–™å¤¾ï¼Œå¾€ä¸Šæ‰¾ .env
env_path = Path(__file__).parent / ".env"
load_dotenv(dotenv_path=env_path)

# === è®€å– Gemini è¨­å®šï¼ˆå–ä»£åŸæœ¬ Azure OpenAIï¼‰ ===
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_BASE_URL = os.getenv(
    "GEMINI_BASE_URL",
    "https://generativelanguage.googleapis.com/v1beta/openai/"
)
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")

try:
    llm_client = OpenAI(
        api_key=GEMINI_API_KEY,
        base_url=GEMINI_BASE_URL,
    )
except Exception as e:
    print(f"âŒ Gemini Client åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr)
    llm_client = None

mcp = FastMCP("product-expert-agent")

# ==========================================
# 2. å®šç¾©å…§éƒ¨å·¥å…· (Internal Tools)
# ==========================================
async def tool_rag_search_product(
    user_query: str,
    card_name: str | None = None,
    top_k: int = 5
) -> str:
    """
    ç”¨ RAG æŸ¥è©¢ä¿¡ç”¨å¡ç”¢å“è³‡è¨Šï¼Œå›å‚³ç›¸é—œ chunksã€‚
    """
    print(
        f"   âš™ï¸ [Internal Tool] RAG search | q={user_query}, card={card_name}",
        file=sys.stderr
    )

    # ğŸ” æ ¹æ“šå•é¡Œå…§å®¹ï¼Œèª¿æ•´æŸ¥è©¢ç­–ç•¥
    # å¦‚æœæ˜¯åœ¨å•ã€Œå›é¥‹ / æ¬Šç›Š / é€šè·¯ã€ï¼Œå„ªå…ˆæŠ“ benefit_schemeï¼Œtop_k é–‹å¤§ä¸€é»
    lower_q = user_query.lower()
    is_benefit_query = any(
        kw in user_query for kw in ["å›é¥‹", "æ¬Šç›Š", "é€šè·¯", "æ–¹æ¡ˆ"]
    )

    if is_benefit_query:
        effective_top_k = max(top_k, 20)
        doc_type = "benefit_scheme"
    else:
        effective_top_k = top_k
        doc_type = None
        
    my_metadata = {
    "card_name": card_name,
    "doc_type": doc_type
}
     # ç¢ºä¿ç”¨çš„æ˜¯ä½ ç¾åœ¨æœ‰ card_filter çš„ç‰ˆæœ¬

    results = search_chunks(
        query=user_query,
        top_k=effective_top_k,
        metadata_filter=my_metadata
    )

    print(results)
    return results 


async def tool_calculate_installment(amount: int, months: int) -> str:
    """æ¨¡æ“¬è¨ˆç®—ï¼šåˆ†æœŸä»˜æ¬¾è©¦ç®— (ä¸å«åˆ©æ¯ç°¡å–®é™¤æ³•)"""
    print(f"   âš™ï¸ [Internal Tool] åŸ·è¡Œ tool_calculate_installment (ç®—åˆ†æœŸ) | åƒæ•¸: {amount} / {months}", file=sys.stderr)
    if months <= 0:
        return json.dumps({"error": "æœŸæ•¸å¿…é ˆå¤§æ–¼0"})
    
    per_month = int(amount / months)
    return json.dumps({
        "total_amount": amount,
        "months": months,
        "payment_per_month": per_month,
        "note": "æ­¤ç‚ºé ä¼°å€¼ï¼Œå¯¦éš›é‡‘é¡ä»¥å¸³å–®ç‚ºæº–"
    })

# ==========================================
# 3. å®šç¾©å·¥å…·æ¸…å–® (JSON Schema)
# ==========================================
INTERNAL_TOOLS_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "tool_rag_search_product",
            "description": "ç”¨ RAG æŸ¥è©¢ä¿¡ç”¨å¡ç”¢å“è³‡è¨Šï¼Œå¾ chunks ä¸­æ‰¾å¹´è²»ã€æ¬Šç›Šã€è³‡æ ¼ç­‰å…§å®¹ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "user_query": {
                        "type": "string",
                        "description": "ä½¿ç”¨è€…å•é¡ŒåŸæ–‡ï¼Œä¾‹å¦‚ï¼š'åœ‹æ³°ä¸–è¯ä¸–ç•Œå¡æ©Ÿå ´æ¥é€è³‡æ ¼æ˜¯ä»€éº¼ï¼Ÿ'"
                    },
                    "card_name": {
                        "type": "string",
                        "description": "è‹¥å·²çŸ¥è¦æŸ¥çš„å¡ç‰‡åç¨±å°±å¡«ï¼Œå¦å‰‡å¯ç•™ç©ºï¼Œå¯èƒ½çš„å¡ç‰‡åç¨±æœ‰ï¼šåœ‹æ³°CUBEå¡, åœ‹æ³°ä¸–è¯ä¸–ç•Œå¡, åœ‹æ³°è¦çš®è³¼ç‰©è¯åå¡åŠåœ‹æ³°äºæ´²è¬é‡Œé€šè¯åå¡å››å€‹å¯èƒ½",
                        "nullable": True
                    },
                    "top_k": {
                        "type": "integer",
                        "description": "è¦å–å›æœ€ç›¸é—œçš„å¹¾ç­†è³‡æ–™",
                        "default": 5
                    }
                },
                "required": ["user_query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "tool_calculate_installment",
            "description": "è¨ˆç®—åˆ†æœŸä»˜æ¬¾æ¯æœŸæ‡‰ç¹³é‡‘é¡ã€‚",
            "parameters": {
                "type": "object",
                "properties": {
                    "amount": {"type": "integer", "description": "ç¸½é‡‘é¡"},
                    "months": {"type": "integer", "description": "åˆ†æœŸæœŸæ•¸"}
                },
                "required": ["amount", "months"]
            }
        }
    }
]


PRODUCT_SYSTEM_PROMPT = """
ä½ æ˜¯åœ‹æ³°ä¸–è¯éŠ€è¡Œçš„ã€Œä¿¡ç”¨å¡ç”¢å“å°ˆå®¶ã€ã€‚
ä½ çš„ä»»å‹™æ˜¯æä¾›ç²¾ç¢ºçš„ç”¢å“æ•¸æ“šèˆ‡è©¦ç®—æœå‹™ã€‚

# å¯ç”¨å·¥å…·ï¼š
- `tool_rag_search_product`: å¾å…§éƒ¨ RAG è³‡æ–™åº«æŸ¥è©¢ä¿¡ç”¨å¡ç”¢å“è³‡è¨Š
  ï¼ˆåŒ…å«å¹´è²»ã€å“©ç¨‹/é»æ•¸å›é¥‹ã€ç”³è¾¦è³‡æ ¼ã€é¦–åˆ·ç¦®ã€æ©Ÿå ´æ¥é€ã€è²´è³“å®¤ã€æµ·å¤–æ¼«éŠç­‰ï¼‰ã€‚
- `tool_calculate_installment`: å¹«å®¢æˆ¶ç®—åˆ†æœŸé‡‘é¡ã€‚

# ä½¿ç”¨åŸå‰‡ï¼š
- åªè¦æ˜¯ã€Œå›ºå®šè¦å‰‡ã€æˆ–ã€Œæ•¸å­—å‹è³‡è¨Šã€ï¼ˆå¹´è²»ã€é–€æª»ã€å›é¥‹å€ç‡ã€æ¬¡æ•¸ï¼‰éƒ½æ‡‰å„ªå…ˆç”¨ RAG å·¥å…·æŸ¥è©¢ï¼Œ
  åš´ç¦æ†‘ç©ºæé€ ã€‚
- å›ç­”æ™‚è«‹æ•´ç†æŸ¥å›ä¾†çš„å…§å®¹ï¼Œä»¥æ¢åˆ—èªªæ˜ï¼Œè®“ä½¿ç”¨è€…æ˜“è®€ã€‚
- å¦‚æœ RAG æŸ¥ä¸åˆ°è³‡æ–™ï¼Œè¦æ˜ç¢ºèªªã€Œç›®å‰è³‡æ–™åº«æ²’æœ‰é€™å¼µå¡çš„è³‡è¨Šã€ã€‚

- âš  ç‰¹åˆ¥æ³¨æ„ï¼š
  è‹¥åŒä¸€å¼µå¡æœ‰å¤šå€‹æ¬Šç›Šæ–¹æ¡ˆï¼ˆä¾‹å¦‚ä¾é¡åˆ¥åˆ†æˆä¸åŒæ–¹æ¡ˆï¼‰ï¼Œ
  ç•¶ä½¿ç”¨è€…è©¢å•ã€Œé€™å¼µå¡çš„å›é¥‹ / æ¬Šç›Šæœ‰å“ªäº›ï¼Ÿã€æ™‚ï¼Œ
  è«‹ç›¡é‡å®Œæ•´åˆ—å‡ºæ‰€æœ‰ä¸»è¦æ–¹æ¡ˆï¼Œè€Œä¸æ˜¯åªé¸å…¶ä¸­ä¸€ã€å…©å€‹ã€‚
"""

# ==========================================
# 4. æ ¸å¿ƒé‚è¼¯å±¤ (ReAct Loop)
# ==========================================
async def _generate_response(user_query: str) -> str:
    if not llm_client:
        return "âŒ ç³»çµ±éŒ¯èª¤ï¼šLLM client æœªåˆå§‹åŒ–ã€‚"

    messages = [
        {"role": "system", "content": PRODUCT_SYSTEM_PROMPT},
        {"role": "user", "content": user_query + "æˆ‘æ˜¯åœ‹æ³°cubeå¡çš„äºŒç´šç”¨æˆ¶"}
    ]

    MAX_TURNS = 5
    current_turn = 0

    try:
        while current_turn < MAX_TURNS:
            current_turn += 1
            
            # 1. å‘¼å« LLMï¼ˆGemini OpenAI-compatibleï¼‰
            response = llm_client.chat.completions.create(
                model=GEMINI_MODEL,
                messages=messages,
                tools=INTERNAL_TOOLS_SCHEMA,
                tool_choice="auto"
            )
            msg = response.choices[0].message
            messages.append(msg)

            # 2. åˆ¤æ–·æ˜¯å¦çµæŸ
            if not msg.tool_calls:
                return msg.content

            # 3. åŸ·è¡Œå·¥å…·
            for tool_call in msg.tool_calls:
                func_name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                
                result_content = ""
                
                if func_name == "tool_rag_search_product":
                    result_content = await tool_rag_search_product(**args)
                elif func_name == "tool_calculate_installment":
                    result_content = await tool_calculate_installment(**args)
                else:
                    result_content = json.dumps({"error": "Unknown tool"})

                # 4. åŠ å…¥å·¥å…·çµæœ
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "name": func_name,
                    "content": str(result_content)
                })
            
        return "æ€è€ƒæ¬¡æ•¸éå¤šï¼Œç„¡æ³•ç”¢ç”Ÿå®Œæ•´å›ç­”ã€‚"

    except Exception as e:
        return f"Agent åŸ·è¡Œç™¼ç”ŸéŒ¯èª¤: {str(e)}"

# ==========================================
# MCP ä»‹é¢å±¤
# ==========================================
@mcp.tool()
async def product_agent(user_query: str) -> str:
    """ã€ç”¢å“å°ˆå®¶å…¥å£ã€‘æ¥æ”¶ä½¿ç”¨è€…çš„å•é¡Œï¼Œé€é LLM èˆ‡å…§éƒ¨å·¥å…·ç”Ÿæˆç”¢å“è³‡è¨Šã€‚"""
    print(f"ğŸ’³ [Product Agent] æ”¶åˆ°è«‹æ±‚ (MCP) | Query: {user_query}", file=sys.stderr)
    return await _generate_response(user_query)

# ==========================================
# Local æ¸¬è©¦å±¤
# ==========================================
async def local_chat_loop():
    print("\nğŸ’³ --- ç”¢å“å°ˆå®¶ Agent (æœ¬åœ°æ¸¬è©¦æ¨¡å¼) ---")
    print("è¼¸å…¥ 'q' é›¢é–‹ã€‚")
    print("(æ¸¬è©¦æç¤ºï¼šè©¦è‘—å• 'CUBEå¡å¹´è²»å¤šå°‘?' æˆ– 'è²·iPhone 3è¬åˆ†12æœŸè¦ç¹³å¤šå°‘?')")
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ (User): ").strip()
            if user_input.lower() in ['q', 'quit', 'exit']:
                break
            if not user_input:
                continue
            
            print("ğŸ’³ (Agent): æ€è€ƒä¸­...", end="\r")
            reply = await _generate_response(user_input)
            print(f"ğŸ’³ (Agent): {reply}")
            
        except KeyboardInterrupt:
            break
    print("\nBye!")

# ==========================================
# ä¸»ç¨‹å¼å…¥å£
# ==========================================
if __name__ == "__main__":
    if "--local" in sys.argv:
        if sys.platform.startswith('win'):
             asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
        asyncio.run(local_chat_loop())
    else:
        print("ğŸ’³ Product Agent Server starting...", file=sys.stderr)
        mcp.run()